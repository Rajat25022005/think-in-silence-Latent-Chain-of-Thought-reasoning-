<div align="center">

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—      â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•      â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•       â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•—       â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—      â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•  â•šâ•â•     â•šâ•â•â•šâ•â•  â•šâ•â•â•â•

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â•â•šâ•â•â•â•â•â•â•
```

**A model that reasons in pure latent space.**  
No tokens. No chain-of-thought labels. No reinforcement learning.

<br>

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2+-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)](https://pytorch.org)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-4.38+-FFD21E?style=flat-square)](https://huggingface.co)
[![License](https://img.shields.io/badge/License-MIT-22C55E?style=flat-square)](LICENSE)

</div>

---

## The Idea

Most language models think out loud.

```
Q: A train travels 60 mph for 2 hours. How far?
A: The train travels 60 Ã— 2 = 120 miles.       â† reasoning exposed as tokens
   The answer is 120.
```

**think-in-silence** doesn't. Instead of generating intermediate steps as text, it runs *K learned thought vectors* through a recurrent cross-attention chain â€” entirely in embedding space â€” before predicting an answer:

```
Q: [encoded â†’ 256-dim context vector]
       â”‚
       â–¼
   hâ‚€  â†’  ThoughtBlock(hâ‚€, ctx)  â†’  hâ‚        â† silent step 1
   hâ‚  â†’  ThoughtBlock(hâ‚, ctx)  â†’  hâ‚‚        â† silent step 2
   hâ‚‚  â†’  ThoughtBlock(hâ‚‚, ctx)  â†’  hâ‚ƒ        â† silent step 3
                    ...
   hâ‚–â‚‹â‚ â†’ ThoughtBlock(hâ‚–â‚‹â‚, ctx) â†’ hâ‚–       â† silent step K
       â”‚
       â–¼
   Predictor MLP  â†’  pred âˆˆ â„Â²âµâ¶
       â”‚
   MSE(pred, answer_embedding)                  â† JEPA objective
```

Trained only on `(question, answer)` pairs. **No reasoning traces. No RL. No decoder.**

---

## Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  Question          â”‚  STUDENT  (trained by backprop)             â”‚
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Enc â”€â”¤                                             â”œâ”€â”€â–º Predictor â”€â”€â–º pred
              (ğŸ”’)  â”‚  ThoughtModule: hâ‚€â†’hâ‚â†’hâ‚‚â†’...â†’hâ‚–              |         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
                                                                            â”‚  MSE
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
  Answer            â”‚  TEACHER  (EMA, no gradients)               â”‚         â”‚
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Enc â”€â”¤                                             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
              (ğŸ”’)  â”‚  sentence_emb âˆˆ â„Â²âµâ¶                        |
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ThoughtBlock** â€” one reasoning step:
```
h  â†’  LayerNorm  â†’  Self-Attention(h, h)      +  h    â† refine existing thought
h  â†’  LayerNorm  â†’  Cross-Attention(h, ctx)   +  h    â† read question context
h  â†’  LayerNorm  â†’  FFN(h)                    +  h    â† mix
```

The EMA teacher (momentum 0.990 â†’ 0.9999, cosine schedule) provides stable regression targets, preventing collapse without contrastive negatives or RL rewards.

---

## Key Experiments

### 1 â€” Think Time = Performance

Evaluate the **same checkpoint** at K = 1, 2, 4, 8, 16 steps:

| Reasoning Steps | Recall@1 | Recall@5 |
|:---:|:---:|:---:|
| K = 1  | â€” | â€” |
| K = 2  | â€” | â€” |
| K = 4  | â€” | â€” |
| K = 8  | â€” | â€” |
| K = 16 | â€” | â€” |
| K = 0 (no reasoning) | â€” | â€” |

*If the model genuinely reasons, accuracy increases monotonically with K.*  
*Results populate after training. K is a runtime parameter â€” no retraining needed.*

### 2 â€” Thought Trajectory Probing

At each step hâ‚–, a linear probe predicts:
- **Answer type** â€” numeric / yes-no / entity / phrase
- **Question category** â€” math / factual / commonsense / strategy

Reveals what information each reasoning step encodes spontaneously â€” with zero supervision.

### 3 â€” Reconstruction Quality by Step

MSE between `pred` and `answer_emb` at each K:

```
K=1 â†’ MSE = ?     (early, rough prediction)
K=4 â†’ MSE = ?     (developing)
K=8 â†’ MSE = ?     (converged)
```

Decreasing curve = each additional thought step improves the answer prediction.

### 4 â€” t-SNE Thought Trajectories

How does hâ‚€ â†’ hâ‚– move through 2D latent space?  
Distinct category clusters = the model learns different reasoning paths per question type.

---

## Training Data

Five QA datasets, streamed and interleaved â€” no bulk download required:

| Dataset | Task | Size | Weight |
|---------|------|------|--------|
| [HotpotQA](https://hotpotqa.github.io) | Multi-hop factual | 113K | 35% |
| [GSM8K](https://github.com/openai/grade-school-math) | Grade-school math | 8.5K | 20% |
| [CommonsenseQA](https://www.tau-nlp.org/commonsenseqa) | Commonsense MC | 12K | 20% |
| [ARC-Challenge](https://allenai.org/data/arc) | Science exam (hard) | 7.8K | 15% |
| [StrategyQA](https://allenai.org/data/strategyqa) | Yes/no multi-hop | 2.8K | 10% |

No chain-of-thought annotations used anywhere.

---

## Quickstart

```bash
git clone https://github.com/Rajat25022005/think-in-silence
cd think-in-silence
pip install -r requirements.txt
```

**Train** (GCP L4, ~5â€“6 hrs for 100k steps):
```bash
tmux new -s think
bash run.sh
# Ctrl+B, D  to detach
```

**Evaluate a checkpoint:**
```bash
python eval.py --ckpt checkpoints/base/step_0100000.pt --config configs/base.yaml
```

**Evaluate with t-SNE trajectories:**
```bash
python eval.py --ckpt checkpoints/base/final.pt --config configs/base.yaml --tsne
```

**Adjust reasoning steps at inference (no retraining):**
```python
from src.models.lc_thought import LCThought

model = LCThought(n_steps=8)
# load checkpoint...

# Run with 16 steps instead of 8 â€” no retraining needed
out = model(q_ids, q_mask, a_ids, a_mask, n_steps=16)
```

---

## Project Structure

```
think-in-silence/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ thought_block.py      # One reasoning step (self-attn â†’ cross-attn â†’ FFN)
â”‚   â”‚   â”œâ”€â”€ thought_module.py     # K-step recurrent chain with learnable hâ‚€
â”‚   â”‚   â”œâ”€â”€ encoder.py            # Frozen DistilBERT + projection heads
â”‚   â”‚   â””â”€â”€ lc_thought.py         # Full model: student + EMA teacher + predictor
â”‚   â”‚
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ qa_datasets.py        # Streaming interleaved QA loader
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ trainer.py            # Loop, cosine EMA schedule, BF16, checkpointing
â”‚   â”‚
â”‚   â””â”€â”€ eval/
â”‚       â”œâ”€â”€ evaluator.py          # Retrieval, K-scaling, probing, reconstruction
â”‚       â””â”€â”€ visualise.py          # Publication-quality dark-theme plots
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ base.yaml                 # Training config (K=8, proj_dim=256, 100k steps)
â”‚
â”œâ”€â”€ main.py                       # Training entry point
â”œâ”€â”€ eval.py                       # Evaluation entry point
â”œâ”€â”€ run.sh                        # GCP one-command launch
â””â”€â”€ requirements.txt
```

---

## How It Compares

|  | [Quiet-STaR](https://arxiv.org/abs/2403.09629) | [Coconut](https://arxiv.org/abs/2412.06769) | **think-in-silence** |
|--|:--:|:--:|:--:|
| Reasoning medium | Token vocabulary | LM hidden states | **Dedicated latent space** |
| Training signal | REINFORCE (RL) | Token supervision | **JEPA MSE â€” no RL** |
| Reasoning module | Vocab embeddings | LM backbone | **Separate ThoughtModule** |
| CoT labels needed | No | Partial | **No** |
| K adjustable at inference | No | No | **Yes** |
| Backbone modified | Yes | Yes | **No â€” fully frozen** |

---

## Configuration

Key settings in `configs/base.yaml`:

```yaml
model:
  n_steps:        8      # K reasoning steps
  proj_dim:       256    # Latent space dimension
  shared_weights: false  # true = Universal Transformer (1 block, K passes)

training:
  max_steps:    100000   # ~5-6 hrs on L4
  batch_size:   64
  lr:           1.0e-4
  ema_momentum_start: 0.990
  ema_momentum_end:   0.9999
```

---

## Built On

- [T-JEPA](https://github.com/Rajat25022005/self-supervised-text-jepa) â€” predecessor project
- [I-JEPA](https://arxiv.org/abs/2301.08243) â€” Assran et al., Meta AI (2023)
- [Coconut](https://arxiv.org/abs/2412.06769) â€” Hao et al. (2024)
- [Quiet-STaR](https://arxiv.org/abs/2403.09629) â€” Zeiler et al. (2024)
- [HuggingFace Transformers](https://github.com/huggingface/transformers)

---

<div align="center">
<sub>Built by Rajat Malik Â· 2026 Â· MIT License</sub>
</div>
